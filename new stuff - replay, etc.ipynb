{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "from mdp_class import MDP\n",
    "from q_agent_class import Qagent\n",
    "import matplotlib.animation as animation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 4 # add one for terminal state\n",
    "n_actions_max = 2\n",
    "\n",
    "# build T and R for a simple 2 stage choice task\n",
    "\n",
    "Tsas = np.zeros([n_states,n_actions_max,n_states]) \n",
    "avail_actions = np.array([[0,1],[2,3],[4,5],[]])\n",
    "next_states = np.array([1,2,3,3,3,3])\n",
    "n_total_actions = 6\n",
    "T_2d = np.zeros([n_total_actions, n_states])\n",
    "for i in np.arange(n_total_actions):\n",
    "    T_2d[i,next_states[i]] = 1\n",
    "    \n",
    "for s in np.arange(n_states):\n",
    "    this_state_options = avail_actions[s]\n",
    "    this_state_probs = T_2d[this_state_options,:]\n",
    "    Tsas[s,0:np.size(this_state_options),:] = this_state_probs\n",
    "\n",
    "n_actions = np.array([2,2,2,0])\n",
    "\n",
    "#exmdp = MDP(n_states,n_actions,Rsa,Tsas,terminal_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([0, 1]), list([2, 3]), list([4, 5]), list([])], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avail_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  0.1\n",
      "\n",
      "Qhat pre: \n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " single step gain :\n",
      " [[0.00 0.00]\n",
      " [2.31 0.61]\n",
      " [0.10 0.22]\n",
      " [0.00 0.00]]\n",
      "\n",
      " full gain :\n",
      " [[0.00 0.00]\n",
      " [3.47 1.84]\n",
      " [-0.05 0.07]\n",
      " [0.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "# effect of which Q values\n",
    "\n",
    "# low beta\n",
    "Rsa = np.array([[0,0], [-10,5], [2,3],[0,0]])\n",
    "params = {'beta': .1, 'alpha_q': 1, 'gamma': 1}\n",
    "\n",
    "qag = Qagent(params, n_states, n_actions, Tsas = Tsas, Rsa = Rsa)\n",
    "\n",
    "print('beta: ', params['beta'])\n",
    "\n",
    "print('\\nQhat pre: \\n',qag.Q_hat)\n",
    "\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "print('\\n single step gain :\\n', gain_ss)\n",
    "\n",
    "gain_full = qag.comp_gain(which_Q_new = 'full')\n",
    "print('\\n full gain :\\n', gain_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta:  10\n",
      "\n",
      "Qhat pre: \n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " single step gain :\n",
      " [[0.00 0.00]\n",
      " [5.00 2.50]\n",
      " [1.00 1.50]\n",
      " [0.00 0.00]]\n",
      "\n",
      " full gain :\n",
      " [[0.00 0.00]\n",
      " [7.50 7.50]\n",
      " [-0.50 0.50]\n",
      " [0.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "# high beta\n",
    "Rsa = np.array([[0,0], [-10,5], [2,3],[0,0]])\n",
    "params = {'beta': 10, 'alpha_q': 1, 'gamma': 1}\n",
    "qag = Qagent(params, n_states, n_actions, avail_actions, Tsas = Tsas, Rsa = Rsa)\n",
    "\n",
    "print('beta: ', params['beta'])\n",
    "\n",
    "print('\\nQhat pre: \\n',qag.Q_hat)\n",
    "\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "print('\\n single step gain :\\n', gain_ss)\n",
    "\n",
    "gain_full = qag.comp_gain(which_Q_new = 'full')\n",
    "print('\\n full gain :\\n', gain_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Q_hat :\n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " policy: \n",
      " [[0.50 0.50]\n",
      " [0.50 0.50]\n",
      " [0.50 0.50]\n",
      " [0.00 0.00]]\n",
      "\n",
      " single step gain :\n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "R \n",
      " [[    0     0]\n",
      " [-1000     0]\n",
      " [    0     0]\n",
      " [    0     0]]\n",
      "\n",
      " single step gain :\n",
      " [[0.00 0.00]\n",
      " [500.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " applying backup to max gain action:\n",
      " 1 0\n",
      "\n",
      " Q_hat :\n",
      " [[0.00 0.00]\n",
      " [-1000.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " policy: \n",
      " [[0.50 0.50]\n",
      " [0.00 1.00]\n",
      " [0.50 0.50]\n",
      " [0.00 0.00]]\n",
      "\n",
      " gain :\n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "\n",
      " adding in new R \n",
      " [[    0     0]\n",
      " [-1000     5]\n",
      " [    0     0]\n",
      " [    0     0]]\n",
      "\n",
      " gain :\n",
      " [[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "# adding rewards sequentially\n",
    "Rsa = np.array([[0,0], [0,0], [0,0],[0,0]])\n",
    "params = {'beta': .1, 'alpha_q': 1, 'gamma': 1}\n",
    "qag = Qagent(params, n_states, n_actions, avail_actions, Tsas = Tsas, Rsa = Rsa)\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "\n",
    "print('\\n Q_hat :\\n', qag.Q_hat)\n",
    "print('\\n policy: \\n', qag.comp_pi(qag.Q_hat))\n",
    "print('\\n single step gain :\\n', gain_ss)\n",
    "\n",
    "qag.set_Rsa(1,0,-1000)\n",
    "print('R \\n', qag.Rsa)\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "print('\\n single step gain :\\n', gain_ss)\n",
    "\n",
    "(s,a) = np.unravel_index(np.argmax(gain_ss, axis=None), gain_ss.shape)\n",
    "print('\\n applying backup to max gain action:\\n' ,s,a)\n",
    "qag.backup_Q(s,a,reset=True)\n",
    "print('\\n Q_hat :\\n', qag.Q_hat)\n",
    "print('\\n policy: \\n', qag.comp_pi(qag.Q_hat))\n",
    "\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "print('\\n gain :\\n', gain_ss)\n",
    "\n",
    "qag.set_Rsa(1,1,5)\n",
    "print('\\n adding in new R \\n', qag.Rsa)\n",
    "\n",
    "gain_ss = qag.comp_gain(which_Q_new = 'single_step')\n",
    "print('\\n gain :\\n', gain_ss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qag = Qagent(params, n_states, n_actions, avail_actions, Tsas = Tsas, Rsas = Rsas)\n",
    "choices = ['L', 'R']\n",
    "\n",
    "n_episodes = 3\n",
    "env = exmdp\n",
    "ag = qag\n",
    "max_step = 10\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    #print ('trial: ', i)\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    S = env.reset(env.start_state)\n",
    "\n",
    "    while j < max_step:\n",
    "            \n",
    "        # increase counter\n",
    "        j += 1\n",
    "        \n",
    "        # sample action given by pi for state S\n",
    "        a = qag.sample_action(S)\n",
    "        \n",
    "        # take action A, observe s1, r, terminal?\n",
    "        S_prime,r,nchoices,d = env.step(a)\n",
    "        \n",
    "        #print(ag.Q_hat)\n",
    "        #print('S:',S, 'a:',choices[a],'Sp',S_prime,'r', r)\n",
    "        \n",
    "        # update model\n",
    "        ag.update_Qlearn(S,a,r,S_prime)\n",
    "        \n",
    "\n",
    "        # update S\n",
    "        S = S_prime;\n",
    "            \n",
    "        if d == True:\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_episode(env, choice_vec, max_step):\n",
    "\n",
    "    d = False\n",
    "    j = 0\n",
    "    S = env.reset(env.start_state)\n",
    "\n",
    "    while j < max_step:\n",
    "            \n",
    "        # increase counter\n",
    "        j += 1\n",
    "        \n",
    "        # sample action given by pi for state S\n",
    "        a = choice_vec[j-1]\n",
    "        \n",
    "        # take action A, observe s1, r, terminal?\n",
    "        S_prime,r,nchoices,d = env.step(a)\n",
    "        \n",
    "        print(S,a,r,S_prime)\n",
    "\n",
    "        # update S\n",
    "        S = S_prime;\n",
    "            \n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exmdp = MDP(n_states,n_actions,Rsas,Tsas,terminal_states)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
