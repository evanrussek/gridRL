{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "\n",
    "from mdp_class import MDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qagent():\n",
    "    def __init__(self,params,n_states, n_actions, available_actions, Tsas = [], Rsas = [], Rs = []):\n",
    "        n_actions_max = np.max(n_actions)\n",
    "        n_state_actions = np.sum(n_actions)\n",
    "        \n",
    "        self.n_actions_max = n_actions_max # max number of actions in a single state, scalar\n",
    "        self.n_actions = n_actions # n actions in each state, a list\n",
    "        self.available_actions = available_actions # which actions are available in each state\n",
    "        self.n_state_actions = np.sum(n_actions)\n",
    "        \n",
    "        self.params = params\n",
    "        self.n_states = n_states\n",
    "        self.sa_list = [(s,j) for s in np.arange(n_states) for j in range(n_actions[s])] # list of state,action pairs\n",
    "        \n",
    "        self.Q_hat = np.zeros([n_states, n_actions_max])\n",
    "        self.e = np.zeros(self.n_state_actions) # traces for each state (want this to be for each action , deal with later)\n",
    "        self.S = 'S'\n",
    "        self.S_prime = 'S'\n",
    "        \n",
    "        self.Tsas = Tsas\n",
    "        self.Rsa = Rsas[:,:,0]\n",
    "        self.Rs = Rs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.Q_hat = np.zeros([self.n_states, self.n_actions_max])\n",
    "        self.S = 'S'\n",
    "        self.S_prime = 'S'\n",
    "        \n",
    "    def comp_choice_probs_SM(self,s, Q_hat = []): \n",
    "        n_choices_s = self.n_actions[s]\n",
    "        # apply softmax decision rule to the state\n",
    "        choice_probs = np.zeros([self.n_actions_max])\n",
    "        if len(Q_hat) == 0:\n",
    "            Q_hat = self.Q_hat.copy()\n",
    "     #   import pdb; pdb.set_trace() \n",
    "        Q_hat_s = Q_hat[s,0:n_choices_s]\n",
    "        real_choice_probs = np.exp(self.params['beta']*Q_hat_s)/np.sum(np.exp(self.params['beta']*Q_hat_s))\n",
    "        choice_probs[0:n_choices_s] = real_choice_probs\n",
    "        return choice_probs\n",
    "        \n",
    "    def sample_action(self,s, Q_hat = []): # can add later what choice rule to use\n",
    "        # sample an action under the current Q value function\n",
    "        # build Q values for each action\n",
    "       # import pdb; pdb.set_trace() \n",
    "        choice_probs = self.comp_choice_probs_SM(s)\n",
    "        return nr.choice(np.arange(len(choice_probs)), p = choice_probs)\n",
    "    \n",
    "    def update_Qlearn(self, s, a, r, s_prime, Q_hat = [], reset = True):\n",
    "        # try varying - do expected sarsa backup\n",
    "        # \n",
    "        if len(Q_hat) == 0:\n",
    "            Q_hat_new = self.Q_hat.copy()\n",
    "        else:\n",
    "            Q_hat_new = Q_hat.copy()\n",
    "            \n",
    "        #if s_prime == 7:\n",
    "            #mport pdb; pdb.set_trace()\n",
    "            \n",
    "        if self.n_actions[s_prime] == 0:\n",
    "            # terminal state\n",
    "            target = self.params['gamma']*r\n",
    "        else:\n",
    "            target = self.params['gamma']*(r + np.max(Q_hat_new[s_prime,0:self.n_actions[s_prime]]))\n",
    "            \n",
    "        Q_hat_new[s,a] = (1 - self.params['alpha_q'])*Q_hat_new[s,a] + self.params['alpha_q']*target\n",
    "        \n",
    "        if reset:\n",
    "            self.Q_hat = Q_hat_new\n",
    "            \n",
    "        return Q_hat_new\n",
    "    \n",
    "    def backup_Q(self,s,a, Q_hat = [], reset = True):\n",
    "        if len(Q_hat) == 0:\n",
    "            Q_hat_new = self.Q_hat.copy()\n",
    "        else:\n",
    "            Q_hat_new = Q_hat.copy()\n",
    "        # does a single value iteration / bellman backup to Q\n",
    "        V_hat = [np.max(Q_hat_new[s_prime,0:self.n_actions[s_prime]]) if self.n_actions[s_prime] > 0 else 0 for s_prime in np.arange(self.n_states)]\n",
    "        Q_hat_new[s,a] = self.Rsa[s,a] + np.dot(self.Tsas[s,a,:],V_hat)\n",
    "        \n",
    "        if reset:\n",
    "            self.Q_hat = Q_hat_new\n",
    "        \n",
    "    def comp_pi(self, Q_hat = []):\n",
    "        if len(Q_hat) == 0:\n",
    "            Q_hat = self.Q_hat.copy()\n",
    "        pol_mtx = np.array([self.comp_choice_probs_SM(s,Q_hat) for s in range(self.n_states)])\n",
    "        return pol_mtx\n",
    "    \n",
    "    def comp_Tss(self, pol_mtx, Tsas):\n",
    "        return np.array([np.dot(pol_mtx[s,:],Tsas[s,::]) for s in np.arange(self.n_states)])\n",
    "        \n",
    "    def comp_SR(self,pol_mtx,Tss):\n",
    "        return np.linalg.inv(np.eye(self.n_states) - self.params['gamma']*Tss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [-10.00 -10.00]\n",
      " [5.00 5.00]\n",
      " [3.00 3.00]\n",
      " [2.00 2.00]\n",
      " [0.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "n_states = 8\n",
    "n_actions_max = 2\n",
    "\n",
    "\n",
    "# build T and R for a simple 2 stage choice task\n",
    "\n",
    "Tsas = np.zeros([n_states,n_actions_max,n_states]) \n",
    "\n",
    "avail_actions = np.array([[0,1],[2,3],[4,5],[6],[7],[8],[9],[]])\n",
    "next_states = np.array([1,2,3,4,5,6,7,7,7,7])\n",
    "n_total_actions = 10\n",
    "\n",
    "T_2d = np.zeros([n_total_actions, n_states])\n",
    "for i in np.arange(n_total_actions):\n",
    "    T_2d[i,next_states[i]] = 1\n",
    "    \n",
    "for s in np.arange(n_states):\n",
    "    this_state_options = avail_actions[s]\n",
    "    this_state_probs = T_2d[this_state_options,:]\n",
    "    Tsas[s,0:np.size(this_state_options),:] = this_state_probs\n",
    "    \n",
    "\n",
    "# number of available actions per state\n",
    "n_actions = np.array([2,2,2,1,1,1,1,0])\n",
    "\n",
    "Rs = np.array([0,0,0,-10, 5, 3, 2,0])\n",
    "Rsas = np.zeros([n_states,n_actions_max,n_states])\n",
    "\n",
    "# get the reward when you leave the state\n",
    "for s in np.arange(n_states):\n",
    "    Rsas[s,:,:] = Rs[s]\n",
    "\n",
    "terminal_states = np.array([7])\n",
    "\n",
    "exmdp = MDP(n_states,n_actions,Rsas,Tsas,terminal_states)\n",
    "\n",
    "params = {'beta': .1, 'alpha_q': 1, 'gamma': 1}\n",
    "\n",
    "qag = Qagent(params, n_states, n_actions, avail_actions, Tsas = Tsas, Rsas = Rsas)\n",
    "\n",
    "\n",
    "print(qag.Rsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "[[0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [-10.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n",
      "[[0.00 0.00]\n",
      " [-10.00 0.00]\n",
      " [0.00 0.00]\n",
      " [-10.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]\n",
      " [0.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "qag = Qagent(params, n_states, n_actions, avail_actions, Tsas = Tsas, Rsas = Rsas)\n",
    "# test backups\n",
    "print(qag.Q_hat)\n",
    "qag.backup_Q(3,0)\n",
    "print(qag.Q_hat)\n",
    "qag.backup_Q(1,0)\n",
    "print(qag.Q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qag.Q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "qag = Qagent(params, n_states, n_actions, avail_actions)\n",
    "choices = ['L', 'R']\n",
    "\n",
    "n_episodes = 50\n",
    "env = exmdp\n",
    "ag = qag\n",
    "max_step = 10\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    #print ('trial: ', i)\n",
    "    \n",
    "    d = False\n",
    "    j = 0\n",
    "    S = env.reset(env.start_state)\n",
    "\n",
    "    while j < max_step:\n",
    "            \n",
    "        # increase counter\n",
    "        j += 1\n",
    "        \n",
    "        # sample action given by pi for state S\n",
    "        a = qag.sample_action(S)\n",
    "        \n",
    "        # take action A, observe s1, r, terminal?\n",
    "        S_prime,r,nchoices,d = env.step(a)\n",
    "        \n",
    "        #print(ag.Q_hat)\n",
    "        #print('S:',S, 'a:',choices[a],'Sp',S_prime,'r', r)\n",
    "        \n",
    "        # update model\n",
    "        ag.update_Qlearn(S,a,r,S_prime)\n",
    "        \n",
    "\n",
    "        # update S\n",
    "        S = S_prime;\n",
    "            \n",
    "        if d == True:\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.00 3.00]\n",
      " [-10.00 5.00]\n",
      " [3.00 2.00]\n",
      " [-10.00 0.00]\n",
      " [5.00 0.00]\n",
      " [3.00 0.00]\n",
      " [2.00 0.00]\n",
      " [0.00 0.00]]\n",
      "[[0.55 0.45]\n",
      " [0.18 0.82]\n",
      " [0.52 0.48]\n",
      " [1.00 0.00]\n",
      " [1.00 0.00]\n",
      " [1.00 0.00]\n",
      " [1.00 0.00]\n",
      " [0.00 0.00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50, 0.50],\n",
       "       [0.48, 0.52],\n",
       "       [0.50, 0.50],\n",
       "       [1.00, 0.00],\n",
       "       [1.00, 0.00],\n",
       "       [1.00, 0.00],\n",
       "       [1.00, 0.00],\n",
       "       [0.00, 0.00]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ag.Q_hat)\n",
    "\n",
    "pol_mtx = np.array([ag.comp_choice_probs_SM(s,ag.Q_hat) for s in range(ag.n_states)])\n",
    "\n",
    "print(pol_mtx)\n",
    "\n",
    "ag.comp_pi(pol_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag.Q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_episode(env, choice_vec, max_step):\n",
    "\n",
    "    d = False\n",
    "    j = 0\n",
    "    S = env.reset(env.start_state)\n",
    "\n",
    "    while j < max_step:\n",
    "            \n",
    "        # increase counter\n",
    "        j += 1\n",
    "        \n",
    "        # sample action given by pi for state S\n",
    "        a = choice_vec[j-1]\n",
    "        \n",
    "        # take action A, observe s1, r, terminal?\n",
    "        S_prime,r,nchoices,d = env.step(a)\n",
    "        \n",
    "        print(S,a,r,S_prime)\n",
    "\n",
    "        # update S\n",
    "        S = S_prime;\n",
    "            \n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exmdp = MDP(n_states,n_actions,Rsas,Tsas,terminal_states)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)\n",
    "a = 0\n",
    "(s,r,nchoices,done) = exmdp.step(a)\n",
    "print(s,r,nchoices,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph(comment='The Round Table')\n",
    "dot  #doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
