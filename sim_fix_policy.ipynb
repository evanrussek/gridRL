{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import random\n",
    "#import itertools\n",
    "import scipy.misc\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('qt5agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib qt \n",
    "#%matplotlib inline \n",
    "import time\n",
    "\n",
    "# gridworldclass that we made\n",
    "from gridworldclass import gameEnv\n",
    "from Agents import SRTD_agent, TDagent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_episode(env, policy_vec, max_step, show):\n",
    "\n",
    "    d = False\n",
    "    j = 0\n",
    "    S = env.reset()\n",
    "\n",
    "    while j < max_step:\n",
    "        \n",
    "        if show == 1:\n",
    "            # draw environment and pause\n",
    "            env.render(ax)\n",
    "            plt.pause(.02)\n",
    "            \n",
    "        # increase counter\n",
    "        j += 1\n",
    "        \n",
    "        # sample action given by pi for state S\n",
    "        a = policy_vec[S]\n",
    "        \n",
    "        # take action A, observe s1, r, terminal?\n",
    "        S_prime,r,d = env.step(a)\n",
    "\n",
    "        # update S\n",
    "        S = S_prime;\n",
    "            \n",
    "        if d == True:\n",
    "            break\n",
    "            \n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build environment\n",
    "wall_x = np.array([]);\n",
    "wall_y = np.array([]);\n",
    "wall_loc = np.array([wall_y, wall_x]).T\n",
    "wall_states = np.zeros(wall_loc.shape[0])\n",
    "\n",
    "#gameEnv(nrows,ncols,reward_loc, reward_mag,wall_loc, start_pos)\n",
    "nrows = 7\n",
    "ncols = 5\n",
    "r_col = 2#np.random.randint(0,ncols,1)[0]\n",
    "s_col = 2#np.random.randint(0,ncols,1)[0]\n",
    "reward_loc = np.array([[0,r_col]])\n",
    "reward_mag = np.array([1])\n",
    "start_pos = np.array([nrows-1,s_col])\n",
    "env = gameEnv(nrows,ncols,reward_loc, reward_mag,wall_loc, start_pos)\n",
    "\n",
    "plt.close('all')\n",
    "f,ax = plt.subplots(1)\n",
    "env.render(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 2, 1, 1, 0, 2, 2, 1, 3, 3, 2, 3, 1, 0, 1, 1, 2, 3, 1, 2, 2,\n",
       "       2, 3, 0, 3, 2, 0, 3, 3, 0, 2, 2, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nstates = env.nrows*env.ncols\n",
    "policy_vec = nr.choice(4, p = [.25,.25,.25,.25], size=nstates)\n",
    "policy_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanrussek/anaconda/lib/python3.5/site-packages/matplotlib/backend_bases.py:2437: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "show = 1\n",
    "\n",
    "nepisodes = 1\n",
    "max_step = 200\n",
    "\n",
    "theta = [.7, .1, .1, .1];\n",
    "\n",
    "for i in range(nepisodes):\n",
    "    log_lik = -1*sim_episode(env, policy_vec, max_step, 1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# starting policy vector\n",
    "k = 4 # number of options per var\n",
    "v = nstates # number of vars\n",
    "dimcats = []\n",
    "start = len(dimcats)\n",
    "dimcats += [(dim,k) for dim in range(start,start+v)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def astep_unif(dimcats,shuffle, q0, logp):\n",
    "    if shuffle:\n",
    "        nr.shuffle(dimcats)\n",
    "\n",
    "    q = np.copy(q0)\n",
    "    logp_curr = sim_episode(q) # this also needs to take in an environment and other stuff # should be evaluate a policy\n",
    "\n",
    "    for dim, k in dimcats:\n",
    "        curr_val, q[dim] = q[dim], sample_except(k, q[dim])\n",
    "        logp_prop = logp(q) # this is where policy evaluation goes \n",
    "        q[dim], accepted = metrop_select(logp_prop - logp_curr, q[dim], curr_val)\n",
    "        if accepted:\n",
    "            logp_curr = logp_prop\n",
    "    return q\n",
    "\n",
    "def sample_except(limit, excluded):\n",
    "    candidate = nr.choice(limit - 1)\n",
    "    if candidate >= excluded:\n",
    "        candidate += 1\n",
    "    return candidate\n",
    "\n",
    "def metrop_select(mr, q, q0):\n",
    "    \"\"\"Perform rejection/acceptance step for Metropolis class samplers.\n",
    "    Returns the new sample q if a uniform random number is less than the\n",
    "    metropolis acceptance rate (`mr`), and the old sample otherwise, along\n",
    "    with a boolean indicating whether the sample was accepted.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mr : float, Metropolis acceptance rate\n",
    "    q : proposed sample\n",
    "    q0 : current sample\n",
    "    Returns\n",
    "    -------\n",
    "    q or q0\n",
    "    \"\"\"\n",
    "    # Compare acceptance ratio to uniform random number\n",
    "    if np.isfinite(mr) and np.log(uniform()) < mr:\n",
    "        return q, True\n",
    "    else:\n",
    "        return q0, False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = nr.choice(k, p = [.25,.25,.25,.25], size=d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) sample policy matrix\n",
    "2) compute V from policy matrix\n",
    "3) compute P(R=1 | V) = Z^-1exp(V(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nstates = env.nrows*env.ncols\n",
    "theta = np.array([.25,.25,.25,.25])\n",
    "policy_vec = nr.choice(4, size= nstates, p = theta)\n",
    "\n",
    "# build environment\n",
    "wall_x = np.array([]);\n",
    "wall_y = np.array([]);\n",
    "wall_loc = np.array([wall_y, wall_x]).T\n",
    "wall_states = np.zeros(wall_loc.shape[0])\n",
    "\n",
    "#gameEnv(nrows,ncols,reward_loc, reward_mag,wall_loc, start_pos)\n",
    "nrows = 7\n",
    "ncols = 5\n",
    "r_col = 2#np.random.randint(0,ncols,1)[0]\n",
    "s_col = 2#np.random.randint(0,ncols,1)[0]\n",
    "reward_loc = np.array([[0,r_col]])\n",
    "reward_mag = np.array([1])\n",
    "start_pos = np.array([nrows-1,s_col])\n",
    "env = gameEnv(nrows,ncols,reward_loc, reward_mag,wall_loc, start_pos)\n",
    "\n",
    "plt.close('all')\n",
    "f,ax = plt.subplots(1)\n",
    "env.render(ax)\n",
    "\n",
    "# get lookahead matrix and reward\n",
    "R = env.R\n",
    "lookaheadmtx = env.lookaheadmtx\n",
    "start_state = env.start_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = False\n",
    "j = 0\n",
    "S = start_state\n",
    "max_step = 100\n",
    "\n",
    "while j < max_step:\n",
    "                    \n",
    "    # increase counter\n",
    "    j += 1\n",
    "        \n",
    "    # sample action given by pi for state S\n",
    "    a = policy_vec[int(S)]\n",
    "        \n",
    "    # take action A, observe s1, r, terminal?\n",
    "    S_prime = lookaheadmtx[int(S),a]\n",
    "    r = R[int(S)]\n",
    "    \n",
    "    if r > 0:\n",
    "        break \n",
    "        \n",
    "    # update S\n",
    "    S = S_prime;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
